{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde5093a",
   "metadata": {},
   "source": [
    "# 📊 Data Structures & Algorithms Performance Analysis\n",
    "\n",
    "This notebook provides comprehensive performance analysis and visualization of our enhanced algorithms.\n",
    "\n",
    "## 🎯 Objectives\n",
    "- Benchmark algorithm performance\n",
    "- Visualize time complexity characteristics\n",
    "- Compare implementations\n",
    "- Demonstrate real-world usage patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be48296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "from enhanced_lru_cache import LRUCache\n",
    "from enhanced_file_finder import FileSearcher\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b00f72",
   "metadata": {},
   "source": [
    "## 🔄 LRU Cache Performance Analysis\n",
    "\n",
    "Let's analyze the performance characteristics of our LRU Cache implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cfc019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_lru_cache() -> pd.DataFrame:\n",
    "    \"\"\"Benchmark LRU Cache operations across different sizes.\"\"\"\n",
    "    capacities = [10, 50, 100, 500, 1000, 5000]\n",
    "    operations_count = 1000\n",
    "    results = []\n",
    "    \n",
    "    for capacity in capacities:\n",
    "        cache = LRUCache(capacity)\n",
    "        \n",
    "        # Benchmark SET operations\n",
    "        start_time = time.perf_counter()\n",
    "        for i in range(operations_count):\n",
    "            cache.set(i, f\"value_{i}\")\n",
    "        set_time = time.perf_counter() - start_time\n",
    "        \n",
    "        # Benchmark GET operations\n",
    "        start_time = time.perf_counter()\n",
    "        for i in range(operations_count):\n",
    "            cache.get(random.randint(0, operations_count - 1))\n",
    "        get_time = time.perf_counter() - start_time\n",
    "        \n",
    "        results.append({\n",
    "            'capacity': capacity,\n",
    "            'set_time_ms': set_time * 1000,\n",
    "            'get_time_ms': get_time * 1000,\n",
    "            'set_ops_per_sec': operations_count / set_time,\n",
    "            'get_ops_per_sec': operations_count / get_time\n",
    "        })\n",
    "        \n",
    "        print(f\"Capacity {capacity:4d}: SET {set_time*1000:6.2f}ms, GET {get_time*1000:6.2f}ms\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run benchmark\n",
    "lru_results = benchmark_lru_cache()\n",
    "lru_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca33a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LRU Cache Performance\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Time vs Capacity\n",
    "ax1.plot(lru_results['capacity'], lru_results['set_time_ms'], 'o-', label='SET operations', linewidth=2, markersize=8)\n",
    "ax1.plot(lru_results['capacity'], lru_results['get_time_ms'], 's-', label='GET operations', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Cache Capacity')\n",
    "ax1.set_ylabel('Time (milliseconds)')\n",
    "ax1.set_title('LRU Cache: Time vs Capacity')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Operations per second\n",
    "ax2.bar(lru_results['capacity'], lru_results['set_ops_per_sec'], alpha=0.7, label='SET ops/sec')\n",
    "ax2.bar(lru_results['capacity'], lru_results['get_ops_per_sec'], alpha=0.7, label='GET ops/sec')\n",
    "ax2.set_xlabel('Cache Capacity')\n",
    "ax2.set_ylabel('Operations per Second')\n",
    "ax2.set_title('LRU Cache: Throughput Analysis')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Memory usage simulation\n",
    "memory_usage = lru_results['capacity'] * 64  # Assume 64 bytes per entry\n",
    "ax3.plot(lru_results['capacity'], memory_usage, 'ro-', linewidth=2, markersize=8)\n",
    "ax3.set_xlabel('Cache Capacity')\n",
    "ax3.set_ylabel('Memory Usage (bytes)')\n",
    "ax3.set_title('LRU Cache: Memory Usage')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Efficiency comparison\n",
    "efficiency = lru_results['get_ops_per_sec'] / lru_results['capacity']\n",
    "ax4.plot(lru_results['capacity'], efficiency, 'go-', linewidth=2, markersize=8)\n",
    "ax4.set_xlabel('Cache Capacity')\n",
    "ax4.set_ylabel('Efficiency (ops/sec per capacity unit)')\n",
    "ax4.set_title('LRU Cache: Efficiency Analysis')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 LRU Cache Performance Summary:\")\n",
    "print(f\"• Average SET time: {lru_results['set_time_ms'].mean():.2f}ms\")\n",
    "print(f\"• Average GET time: {lru_results['get_time_ms'].mean():.2f}ms\")\n",
    "print(f\"• Peak throughput: {lru_results['get_ops_per_sec'].max():.0f} ops/sec\")\n",
    "print(f\"• Efficiency scales: {'linearly' if efficiency.std() < efficiency.mean() * 0.1 else 'non-linearly'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a92c24a",
   "metadata": {},
   "source": [
    "## 🔍 File Search Performance Analysis\n",
    "\n",
    "Analyzing the performance of our enhanced file search implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327d04cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_file_structure(base_dir: str, num_dirs: int, files_per_dir: int) -> Tuple[str, int]:\n",
    "    \"\"\"Create a test directory structure with specified parameters.\"\"\"\n",
    "    total_files = 0\n",
    "    \n",
    "    for i in range(num_dirs):\n",
    "        dir_path = os.path.join(base_dir, f\"dir_{i}\")\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "        for j in range(files_per_dir):\n",
    "            # Create mix of file types\n",
    "            extensions = ['.txt', '.py', '.c', '.md', '.json']\n",
    "            ext = extensions[j % len(extensions)]\n",
    "            file_path = os.path.join(dir_path, f\"file_{j}{ext}\")\n",
    "            Path(file_path).touch()\n",
    "            total_files += 1\n",
    "    \n",
    "    return base_dir, total_files\n",
    "\n",
    "def benchmark_file_search() -> pd.DataFrame:\n",
    "    \"\"\"Benchmark file search across different directory sizes.\"\"\"\n",
    "    test_configs = [\n",
    "        (10, 10),    # 100 files\n",
    "        (20, 15),    # 300 files\n",
    "        (30, 20),    # 600 files\n",
    "        (50, 20),    # 1000 files\n",
    "        (100, 25),   # 2500 files\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    searcher = FileSearcher()\n",
    "    \n",
    "    for num_dirs, files_per_dir in test_configs:\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Create test structure\n",
    "            _, total_files = create_test_file_structure(temp_dir, num_dirs, files_per_dir)\n",
    "            \n",
    "            # Benchmark single extension search\n",
    "            start_time = time.perf_counter()\n",
    "            txt_files = searcher.find_files('.txt', temp_dir)\n",
    "            single_search_time = time.perf_counter() - start_time\n",
    "            \n",
    "            # Benchmark multiple extension search\n",
    "            start_time = time.perf_counter()\n",
    "            multi_results = searcher.find_multiple_extensions(\n",
    "                {'.txt', '.py', '.c'}, temp_dir\n",
    "            )\n",
    "            multi_search_time = time.perf_counter() - start_time\n",
    "            \n",
    "            results.append({\n",
    "                'total_files': total_files,\n",
    "                'directories': num_dirs,\n",
    "                'files_per_dir': files_per_dir,\n",
    "                'single_search_ms': single_search_time * 1000,\n",
    "                'multi_search_ms': multi_search_time * 1000,\n",
    "                'found_files': len(txt_files),\n",
    "                'files_per_ms': total_files / (single_search_time * 1000)\n",
    "            })\n",
    "            \n",
    "            print(f\"Files: {total_files:4d}, Single: {single_search_time*1000:6.2f}ms, Multi: {multi_search_time*1000:6.2f}ms\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run file search benchmark\n",
    "file_search_results = benchmark_file_search()\n",
    "file_search_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab2eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize File Search Performance\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Search time vs number of files\n",
    "ax1.plot(file_search_results['total_files'], file_search_results['single_search_ms'], \n",
    "         'o-', label='Single Extension', linewidth=2, markersize=8)\n",
    "ax1.plot(file_search_results['total_files'], file_search_results['multi_search_ms'], \n",
    "         's-', label='Multiple Extensions', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Total Files')\n",
    "ax1.set_ylabel('Search Time (milliseconds)')\n",
    "ax1.set_title('File Search: Time vs File Count')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Throughput analysis\n",
    "ax2.bar(file_search_results['total_files'], file_search_results['files_per_ms'], alpha=0.7)\n",
    "ax2.set_xlabel('Total Files')\n",
    "ax2.set_ylabel('Files Processed per Millisecond')\n",
    "ax2.set_title('File Search: Throughput Analysis')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Linear complexity verification\n",
    "# Fit linear trend line\n",
    "z = np.polyfit(file_search_results['total_files'], file_search_results['single_search_ms'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax3.plot(file_search_results['total_files'], file_search_results['single_search_ms'], 'ro', markersize=8, label='Actual')\n",
    "ax3.plot(file_search_results['total_files'], p(file_search_results['total_files']), 'b--', linewidth=2, label='Linear Fit')\n",
    "ax3.set_xlabel('Total Files')\n",
    "ax3.set_ylabel('Search Time (milliseconds)')\n",
    "ax3.set_title('File Search: Linear Complexity Verification')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Efficiency by directory structure\n",
    "efficiency = file_search_results['total_files'] / file_search_results['single_search_ms']\n",
    "ax4.scatter(file_search_results['directories'], efficiency, s=100, alpha=0.7)\n",
    "ax4.set_xlabel('Number of Directories')\n",
    "ax4.set_ylabel('Efficiency (files/ms)')\n",
    "ax4.set_title('File Search: Efficiency vs Directory Count')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 File Search Performance Summary:\")\n",
    "print(f\"• Average search time: {file_search_results['single_search_ms'].mean():.2f}ms\")\n",
    "print(f\"• Peak throughput: {file_search_results['files_per_ms'].max():.1f} files/ms\")\n",
    "print(f\"• Linear complexity: R² = {np.corrcoef(file_search_results['total_files'], file_search_results['single_search_ms'])[0,1]**2:.3f}\")\n",
    "print(f\"• Multi-extension overhead: {(file_search_results['multi_search_ms'].mean() / file_search_results['single_search_ms'].mean() - 1) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0a63f9",
   "metadata": {},
   "source": [
    "## ⚡ Square Root Algorithm Analysis\n",
    "\n",
    "Comparing different square root computation approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f255d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqrt_newton_method(x: int, precision: float = 1e-10) -> float:\n",
    "    \"\"\"Newton's method for square root (for comparison).\"\"\"\n",
    "    if x < 0:\n",
    "        raise ValueError(\"Cannot compute square root of negative number\")\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    \n",
    "    guess = x / 2\n",
    "    while abs(guess * guess - x) > precision:\n",
    "        guess = (guess + x / guess) / 2\n",
    "    return guess\n",
    "\n",
    "def sqrt_binary_search(x: int) -> int:\n",
    "    \"\"\"Binary search method (integer result).\"\"\"\n",
    "    if x < 0:\n",
    "        raise ValueError(\"Cannot compute square root of negative number\")\n",
    "    if x <= 1:\n",
    "        return x\n",
    "    \n",
    "    left, right = 0, x\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        square = mid * mid\n",
    "        \n",
    "        if square == x:\n",
    "            return mid\n",
    "        elif square < x:\n",
    "            if (mid + 1) * (mid + 1) > x:\n",
    "                return mid\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    return right\n",
    "\n",
    "def benchmark_sqrt_algorithms() -> pd.DataFrame:\n",
    "    \"\"\"Benchmark different square root implementations.\"\"\"\n",
    "    test_numbers = [4, 16, 100, 1000, 10000, 100000, 1000000]\n",
    "    iterations = 1000\n",
    "    results = []\n",
    "    \n",
    "    for num in test_numbers:\n",
    "        # Benchmark binary search\n",
    "        start_time = time.perf_counter()\n",
    "        for _ in range(iterations):\n",
    "            sqrt_binary_search(num)\n",
    "        binary_time = time.perf_counter() - start_time\n",
    "        \n",
    "        # Benchmark Newton's method\n",
    "        start_time = time.perf_counter()\n",
    "        for _ in range(iterations):\n",
    "            sqrt_newton_method(num)\n",
    "        newton_time = time.perf_counter() - start_time\n",
    "        \n",
    "        # Benchmark built-in\n",
    "        start_time = time.perf_counter()\n",
    "        for _ in range(iterations):\n",
    "            int(num ** 0.5)\n",
    "        builtin_time = time.perf_counter() - start_time\n",
    "        \n",
    "        results.append({\n",
    "            'number': num,\n",
    "            'binary_time_us': binary_time * 1000000 / iterations,\n",
    "            'newton_time_us': newton_time * 1000000 / iterations,\n",
    "            'builtin_time_us': builtin_time * 1000000 / iterations,\n",
    "            'binary_result': sqrt_binary_search(num),\n",
    "            'newton_result': int(sqrt_newton_method(num)),\n",
    "            'builtin_result': int(num ** 0.5)\n",
    "        })\n",
    "        \n",
    "        print(f\"Number {num:7d}: Binary {binary_time*1000000/iterations:6.2f}μs, Newton {newton_time*1000000/iterations:6.2f}μs\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run sqrt benchmark\n",
    "sqrt_results = benchmark_sqrt_algorithms()\n",
    "sqrt_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e82ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Square Root Algorithm Performance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Performance comparison\n",
    "x_pos = np.arange(len(sqrt_results))\n",
    "width = 0.25\n",
    "\n",
    "ax1.bar(x_pos - width, sqrt_results['binary_time_us'], width, label='Binary Search', alpha=0.8)\n",
    "ax1.bar(x_pos, sqrt_results['newton_time_us'], width, label=\"Newton's Method\", alpha=0.8)\n",
    "ax1.bar(x_pos + width, sqrt_results['builtin_time_us'], width, label='Built-in (**0.5)', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Test Numbers')\n",
    "ax1.set_ylabel('Time per Operation (microseconds)')\n",
    "ax1.set_title('Square Root Algorithms: Performance Comparison')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels([f'{int(x):,}' for x in sqrt_results['number']], rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')  # Log scale for better comparison\n",
    "\n",
    "# Algorithm complexity visualization\n",
    "log_numbers = np.log10(sqrt_results['number'])\n",
    "ax2.plot(log_numbers, np.log10(sqrt_results['binary_time_us']), 'o-', label='Binary Search', linewidth=2, markersize=8)\n",
    "ax2.plot(log_numbers, np.log10(sqrt_results['newton_time_us']), 's-', label=\"Newton's Method\", linewidth=2, markersize=8)\n",
    "ax2.plot(log_numbers, np.log10(sqrt_results['builtin_time_us']), '^-', label='Built-in', linewidth=2, markersize=8)\n",
    "\n",
    "ax2.set_xlabel('log₁₀(Input Number)')\n",
    "ax2.set_ylabel('log₁₀(Time in microseconds)')\n",
    "ax2.set_title('Square Root Algorithms: Complexity Analysis')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Square Root Algorithm Summary:\")\n",
    "print(f\"• Binary Search avg time: {sqrt_results['binary_time_us'].mean():.2f}μs\")\n",
    "print(f\"• Newton's Method avg time: {sqrt_results['newton_time_us'].mean():.2f}μs\")\n",
    "print(f\"• Built-in avg time: {sqrt_results['builtin_time_us'].mean():.2f}μs\")\n",
    "print(f\"• Fastest algorithm: {'Binary Search' if sqrt_results['binary_time_us'].mean() < sqrt_results['newton_time_us'].mean() else 'Newton Method'}\")\n",
    "\n",
    "# Verify correctness\n",
    "correct_binary = (sqrt_results['binary_result'] == sqrt_results['builtin_result']).all()\n",
    "correct_newton = (sqrt_results['newton_result'] == sqrt_results['builtin_result']).all()\n",
    "print(f\"• Binary search correctness: {'✅ Correct' if correct_binary else '❌ Incorrect'}\")\n",
    "print(f\"• Newton method correctness: {'✅ Correct' if correct_newton else '❌ Incorrect'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1096983b",
   "metadata": {},
   "source": [
    "## 🎯 Key Performance Insights\n",
    "\n",
    "### LRU Cache\n",
    "- **Time Complexity**: Consistent O(1) for both GET and SET operations\n",
    "- **Space Complexity**: O(capacity) with minimal overhead\n",
    "- **Scalability**: Linear memory usage, constant time operations\n",
    "\n",
    "### File Search\n",
    "- **Time Complexity**: O(n) where n is the number of files\n",
    "- **Space Complexity**: O(d) where d is the maximum directory depth\n",
    "- **Optimization**: Multiple extension search reduces redundant traversals\n",
    "\n",
    "### Square Root Algorithms\n",
    "- **Binary Search**: Consistent O(log n) time complexity\n",
    "- **Newton's Method**: Quadratic convergence but more overhead\n",
    "- **Built-in**: Highly optimized native implementation\n",
    "\n",
    "## 🚀 Recommendations\n",
    "\n",
    "1. **LRU Cache**: Excellent for high-frequency access patterns\n",
    "2. **File Search**: Use multiple extension search for better efficiency\n",
    "3. **Square Root**: Binary search optimal for integer results\n",
    "4. **Memory Management**: All implementations use optimal space complexity"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
